# Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨Ø¬Ù„Ø¨ Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ£ÙˆÙ„ Ø¹Ø´Ø± Ø§Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©  Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø²ÙŠØ±Ø© Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ
# Ø§Ù„Ø®Ø·ÙˆØ§Øª:
# 1. ØªØ­Ø¯ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø²ÙŠØ±Ø©
# 2. Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© requests
# 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¹Ø¨Ø± ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø© (status code)
# 4. ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© BeautifulSoup Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
# 5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ 10 Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ±ÙˆØ§Ø¨Ø·Ù‡Ø§ ÙˆØ¹Ø±Ø¶Ù‡Ø§ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· ÙƒØ§Ù…Ù„Ø©

import requests
# Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø²ÙŠØ±Ø©
from bs4 import BeautifulSoup
# 1. ØªØ­Ø¯ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø¬Ø²ÙŠØ±Ø©
url = "https://www.aljazeera.net/"
# 2. Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ù…ÙˆÙ‚Ø¹
response = requests.get(url)
# 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„
if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
 # 4. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø¯Ø¯ CSS
    headlines = soup.select(".gc__title a")# Ù…Ø­Ø¯Ø¯ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    # headlines = soup.select(".article-card__liveblog-title a")# Ù…Ø­Ø¯Ø¯ Ù„Ø¬Ù„Ø¨ Ø§Ø®Ø± Ø§Ù„Ø§Ø®Ø¨Ø§Ø±
    print("ğŸ“° Ten news from Al Jazeera's homepage:\n")
    # print("Latest News from Al_Jazeera: \n")

    for headline in headlines[:10]:
        title = headline.text.strip()
        link = headline['href']

        if not link.startswith("http"):
            link = "https://www.aljazeera.net" + link

        print(f"- {title}\n  ({link})\n")
else:
    print(f"Failed to load the page. Status code: {response.status_code}")
    


print("___________________________________________________________________________________________________")
#________________________________________________________________________________________________________________        

# Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙ‚ÙˆÙ… Ø¨Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø²ÙŠØ±Ø© Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ
# Ø§Ù„Ø®Ø·ÙˆØ§Øª:
# 1. ØªØ­Ø¯ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø²ÙŠØ±Ø©
# 2. Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© requests
# 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¹Ø¨Ø± ÙƒÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø© (status code)
# 4. ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© BeautifulSoup Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
# 5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø·ÙˆÙŠÙ„ Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø·Ù‡Ø§
# 6. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·
# 7. Ø¹Ø±Ø¶ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ØªÙŠ ØªÙ… Ø¬Ù…Ø¹Ù‡Ø§ ÙˆØ·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø·Ù‡Ø§
import requests
from bs4 import BeautifulSoup

# 1. Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø²ÙŠØ±Ø©
# ØªØ­Ø¯ÙŠØ¯ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù†Ù‡Ø§
url = "https://www.aljazeera.net/"

# 2. Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ GET Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
response = requests.get(url)

# 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© (Ø±Ù…Ø² Ø§Ù„Ø­Ø§Ù„Ø© 200 ÙŠØ¹Ù†ÙŠ Ù†Ø¬Ø§Ø­)
if response.status_code == 200:
    # 4. ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # 5. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø·ÙˆÙŠÙ„ (Ù‚Ø¯ ØªÙƒÙˆÙ† Ø¹Ù†Ø§ÙˆÙŠÙ† Ø£Ø®Ø¨Ø§Ø±)
    headlines = []
    for a in soup.find_all("a", href=True):
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¹ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª
        text = a.get_text(strip=True)
        href = a['href']
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©
        if text and len(text) > 30:
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù†Ø³Ø¨ÙŠÙ‹Ø§ØŒ Ù†Ø¶ÙŠÙ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ø¬Ø¹Ù„Ù‡ Ø±Ø§Ø¨Ø· ÙƒØ§Ù…Ù„
            if not href.startswith("http"):
                href = "https://www.aljazeera.net" + href
            headlines.append((text, href))

    # 6. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… dict.fromkeys Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
    unique_headlines = list(dict.fromkeys(headlines))

    # 7. Ø·Ø¨Ø§Ø¹Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ØªÙŠ ØªÙ… Ø¬Ù…Ø¹Ù‡Ø§
    print(f"âœ… Found {len(unique_headlines)} headlines.\n")

    # 8. Ø·Ø¨Ø§Ø¹Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø·Ù‡Ø§
    print("ğŸ“° All news from Al Jazeera's homepage:\n")
    for title, link in unique_headlines:
        print(f"- {title}\n  ({link})\n")

else:
    # Ø­Ø§Ù„Ø© ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©ØŒ Ø·Ø¨Ø§Ø¹Ø© Ø±Ù…Ø² Ø§Ù„Ø­Ø§Ù„Ø©
    print(f"âŒ Failed to load the page. Status code: {response.status_code}")


    






